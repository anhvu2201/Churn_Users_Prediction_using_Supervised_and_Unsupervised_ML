# -*- coding: utf-8 -*-
"""Phan_Anh_Vu_ML_final_project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GEJPZeFv7IVthueOEZXpQ_bkLiRjorhI

# I. **Supervised Learning**

##**1. Preparation**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import warnings
import sys
import time
np.random.seed(42)
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_excel("/content/drive/MyDrive/churn_prediction.xlsx")

"""## **2. EDA**

### **2.1. Data Overview**
"""

data.shape

data.head(5)

data.info()

data.describe()

"""### **2.2. Handle Missing & Duplicate Values**

"""

data.isnull().sum()

missing_cols = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder']

for col in missing_cols:
    median_value = data[col].median()
    data[col].fillna(median_value, inplace=True)

"""- Missing Data:
  - Tenure: 264 values
  - WarehouseToHome: 251 values
  - HourSpendOnApp: 255 values
  - OrderAmoutHikeformLastYear: 265 values
  - CouponUsed: 256 values
  - OrderCount: 258 values
  - DaySinceLastOrder: 307 values
> Action needed: **Filling missing values with respectively median values**
"""

data.nunique()

data.duplicated().any()

"""- Duplicates:
  - Primal Key: CustomerID
  - Duplicates in primal key: None
  - Duplicates in sample: None
> Action needed: **No Action**

### **2.3. Univariate Analysis**
"""

data['CityTier'] = data['CityTier'].astype('object')
data['SatisfactionScore'] = data['SatisfactionScore'].astype('object')

"""- CityTier is a false numeric data without quantitative meanings >> Change back to object data type.
- SatisfactionScore is a false numeric data without quantitative meanings >> Change back to object data type.
- Complain is a false numeric data with quantitative meanings >> Keep data type int64.

#### 2.3.1. Numeric Data
"""

numeric_cols = data.loc[:, data.dtypes != object].columns.tolist()
numeric_cols.remove('CustomerID')
numeric_cols.remove('Churn')

sns.set_theme(style="darkgrid")
for col in numeric_cols:
    print(f"Unique values of {col}: {data[col].nunique()}")
plt.show()
for col in numeric_cols:
    plt.figure(figsize=(16, 8))
    sns.histplot(x=data[col], bins=20, color='#0fbb98')
    plt.title(f'Distribution of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.ylabel('')
plt.show()

for col in numeric_cols:
    plt.figure(figsize=(16, 8))
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.show()

"""- Outliers Detected:
  - Tenure
  - WarehouseToHome
  - HourSpendOnApp
  - NumberOfDeviceRegistered
  - NumberOfAdress
  - OrderAmountHikeFromLastYear
  - CouponUsed
  - OrderCount
  - DaySinceLastOrder
  - CashbackAmount
"""

for col in numeric_cols:
    if col in data.select_dtypes(include=[np.number]).columns:
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        median = data[col].median()
        data[col] = np.where((data[col] < lower_bound) | (data[col] > upper_bound), median, data[col])

"""- Action Needed:
  - All outliers replaced by their respective median values

#### 2.3.2. Category Data
"""

cate_cols = data.loc[:, data.dtypes == object].columns.tolist()
for col in cate_cols:
    print(f"Unique values of {col}: {data[col].nunique()}")
    plt.figure(figsize=(16, 8))
    sns.histplot(x=data[col], color='#0fbb98')
    plt.title(f'Distribution of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.ylabel('')
plt.show()

data['PreferredLoginDevice'] = data['PreferredLoginDevice'].replace('Phone', 'Mobile Phone')
data['PreferedOrderCat'] = data['PreferedOrderCat'].replace('Mobile', 'Mobile Phone')
data['PreferredPaymentMode'] = data['PreferredPaymentMode'].replace('CC', 'Credit Card')
data['PreferredPaymentMode'] = data['PreferredPaymentMode'].replace('COD', 'Cash on Delivery')

"""- As there are 2 values with similar meaning in label PreferredLoginDevice ('Mobile Phone' and 'Phone'), all of them will be change into 'Mobile Phone'.
- As there are 2 values with similar meaning in label PreferredOrderCat ('Mobile' and 'Mobile Phone'), all of them will be change into 'Mobile Phone'.
- As there are 2 values with similar meaning in label PreferredPaymentMode ('CC' and 'Credit Card'), all of them will be change into 'Credit Card'.
- As there are 2 values with similar meaning in label PreferredPaymentMode ('COD' and 'Cash on Delivery'), all of them will be change into 'Cash on Delivery'.

### **2.4. Bivariate & Multivariate Analysis**

#### 2.4.1. Correlation Coefficient
"""

plt.figure(figsize=(16, 8))
numeric_data = data.select_dtypes(include=['number']).drop(columns=['CustomerID'])
sns.heatmap(numeric_data.corr(), cmap='Greens', annot=True)
plt.show()

"""Variables with positive correlation with churn users:
- Notable relationship: Complain
Variables with negative correlation with churn users:
- Medium Relationship: Tenure

#### 2.4.2. Numerical Variables Relationship
"""

fig, ax = plt.subplots(figsize=(16, 8))
sns.boxplot(data=data, x='Churn',y='Complain', color='#0fbb98')
plt.title(f'Boxplot of Complain vs Churn',fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.show()

"""- For non-churn users, most of them have no complain raised in last month.
- For churn users, most of them have complains in the last month.
"""

fig, ax = plt.subplots(figsize=(16, 8))
sns.boxplot(data=data, x='Churn',y='Tenure', color='#0fbb98')
plt.title(f'Boxplot of Tenure vs Churn',fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.show()

"""- For non-churn users, most of them have longer tenure, up to 30 months.
- For churn users, most of them have shorter tenure, from 0 to 15 months.

#### 2.4.3. Categorical Variables Relationship
"""

churn_PLD = data[data['Churn'] == 1].groupby('PreferredLoginDevice').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_PLD = data[data['Churn'] == 0].groupby('PreferredLoginDevice').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
PLD_df = churn_PLD.merge(nchurn_PLD, on = 'PreferredLoginDevice', how = 'outer')
PLD_df = PLD_df.fillna(0)
PLD_df['%'] = (PLD_df['Churn_x'] / (PLD_df['Churn_x'] + PLD_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=PLD_df['PreferredLoginDevice'], y=PLD_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Login Device',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently between login devices of users:
- High percetage: group of users using Computer.
"""

churn_city = data[data['Churn'] == 1].groupby('CityTier').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_city = data[data['Churn'] == 0].groupby('CityTier').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
city_df = churn_city.merge(nchurn_city, on = 'CityTier', how = 'outer')
city_df = city_df.fillna(0)
city_df['%'] = (city_df['Churn_x'] / (city_df['Churn_x'] + city_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=city_df['CityTier'], y=city_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across City Tier',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently between city tier:
- High percetage: group of users in city tier 2 and 3.
"""

churn_payment = data[data['Churn'] == 1].groupby('PreferredPaymentMode').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_payment = data[data['Churn'] == 0].groupby('PreferredPaymentMode').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
payment_df = churn_payment.merge(nchurn_payment, on = 'PreferredPaymentMode', how = 'outer')
payment_df = payment_df.fillna(0)
payment_df['%'] = (payment_df['Churn_x'] / (payment_df['Churn_x'] + payment_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=payment_df['PreferredPaymentMode'], y=payment_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Payment Mode',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently across payment mode used:
- High percetage: Cash on Delivery and E-wallet.
"""

churn_gender = data[data['Churn'] == 1].groupby('Gender').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_gender = data[data['Churn'] == 0].groupby('Gender').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
gender_df = churn_gender.merge(nchurn_gender, on = 'Gender', how = 'outer')
gender_df = gender_df.fillna(0)
gender_df['%'] = (gender_df['Churn_x'] / (gender_df['Churn_x'] + gender_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=gender_df['Gender'], y=gender_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Gender',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute similarly across gender:
- There is no different of churned users between 2 genders.
"""

churn_cat = data[data['Churn'] == 1].groupby('PreferedOrderCat').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_cat = data[data['Churn'] == 0].groupby('PreferedOrderCat').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
cat_df = churn_cat.merge(nchurn_cat, on = 'PreferedOrderCat', how = 'outer')
cat_df = cat_df.fillna(0)
cat_df['%'] = (cat_df['Churn_x'] / (cat_df['Churn_x'] + cat_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=cat_df['PreferedOrderCat'], y=cat_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Category',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently across category:
- High percetage: Fashion and Mobile Phone.
"""

churn_score = data[data['Churn'] == 1].groupby('SatisfactionScore').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_score = data[data['Churn'] == 0].groupby('SatisfactionScore').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
score_df = churn_score.merge(nchurn_score, on = 'SatisfactionScore', how = 'outer')
score_df = score_df.fillna(0)
score_df['%'] = (score_df['Churn_x'] / (score_df['Churn_x'] + score_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=score_df['SatisfactionScore'], y=score_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Satisfaction Score',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently across customers' satisfaction score:
- The churned user rate increases higher as the satisfaction score increases.
"""

churn_mari = data[data['Churn'] == 1].groupby('MaritalStatus').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
nchurn_mari = data[data['Churn'] == 0].groupby('MaritalStatus').agg({'Churn': 'count'}).reset_index().sort_values(by='Churn', ascending=False)
mari_df = churn_mari.merge(nchurn_mari, on = 'MaritalStatus', how = 'outer')
mari_df = mari_df.fillna(0)
mari_df['%'] = (mari_df['Churn_x'] / (mari_df['Churn_x'] + mari_df['Churn_y'])) * 100
fig, ax = plt.subplots(figsize=(16, 8))
sns.barplot(x=mari_df['MaritalStatus'], y=mari_df['%'], ax=ax, color='#0fbb98')
plt.title('Churn User Percentage Across Marital Status',  fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""Churned users distribute differently across marital status:
- High percetage: Single.

## **3. Feature Engineering**

### Select Potential Features
"""

df_model= data[['Tenure','Complain', 'PreferredLoginDevice','CityTier','PreferredPaymentMode','PreferedOrderCat','SatisfactionScore','MaritalStatus','Churn']]
df_model.head()

"""## **4. Feature Transforming**

### Encoding
"""

list_columns = ['PreferredLoginDevice', 'PreferredPaymentMode','PreferedOrderCat','MaritalStatus']
df_encoded = pd.get_dummies(df_model, columns = list_columns)
df_encoded.head()

"""## **5. Model Training**

### **5.1. Split Data Set**
"""

from sklearn.model_selection import train_test_split
x=df_encoded.drop('Churn', axis = 1)
y=df_encoded[['Churn']]
x_train, x_temp, y_train, y_temp = train_test_split(x,y, test_size=0.3, random_state=42)
x_val, x_test, y_val, y_test = train_test_split(x_temp,y_temp, test_size=0.5, random_state=42)
print(f"Number of Data in Train Set: {len(x_train)}")
print(f"Number of Data in Validate Set: {len(x_val)}")
print(f"Number of Data in Test Set: {len(x_test)}")

"""### **5.2. Normalization**"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
x_val_scaled = scaler.transform(x_val)
x_test_scaled = scaler.transform(x_test)

"""### **5.3. Apply Model**

#### Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
clf_rand = RandomForestClassifier(max_depth=15, random_state=0, n_estimators = 100)
clf_rand.fit(x_train_scaled, y_train)
y_ranf_pre_train = clf_rand.predict(x_train_scaled)
y_ranf_pre_val = clf_rand.predict(x_val_scaled)

"""## **6.Model Evaluation**"""

from sklearn.metrics import balanced_accuracy_score, f1_score
from sklearn.metrics import precision_score

"""### **Random Forest**

#### 6.1. Precision
"""

precision_train = precision_score(y_train, y_ranf_pre_train) * 100
precision_val = precision_score(y_val, y_ranf_pre_val) * 100
print(f"Precision on Training: {precision_train:.2f}%")
print(f"Precision on Validating: {precision_val:.2f}%")

"""#### 6.2. F1-Score"""

f1_train = f1_score(y_train, y_ranf_pre_train)
f1_val = f1_score(y_val, y_ranf_pre_val)
print(f"F1 Score on Training: {f1_train:.2f}")
print(f"F1 Score on Validating: {f1_val:.2f}")

"""#### 6.3. Model Fitting"""

balanced_accuracy_train = balanced_accuracy_score(y_train, y_ranf_pre_train)
balanced_accuracy_val = balanced_accuracy_score(y_val, y_ranf_pre_val)
print(balanced_accuracy_train,balanced_accuracy_val)

"""## **7. Improve Model**

### **Random Forest Optimization**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 15, 30],
    'class_weight': ['none', 'balanced']}
clf_rand = RandomForestClassifier(random_state=0)
grid_search = GridSearchCV(clf_rand, param_grid, cv=5, scoring='balanced_accuracy')
grid_search.fit(x_train, y_train)
print("Best Parameters: ", grid_search.best_params_)
best_clf = grid_search.best_estimator_
accuracy = best_clf.score(x_test, y_test)
print("Test set accuracy: ", accuracy)

"""# **II. Unsupervised Learning**

##**1. Preparation**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import warnings
import sys
import time
np.random.seed(42)
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

data_2 = pd.read_excel("/content/drive/MyDrive/churn_prediction.xlsx")
data_uns = data_2[data_2['Churn'] == 1]
data_uns.head()

"""- Only churned users are selected as the unsupervised learning is aimed at this customer segment.

## **2. EDA**

### **2.1. Data Overview**
"""

data_uns.shape

data_uns.head(5)

data_uns.info()

data_uns.describe()

"""### **2.2. Handle Missing & Duplicate Values**

"""

data_uns.isnull().sum()

numeric_data_uns = data_uns.select_dtypes(include=np.number)
data_uns.fillna(numeric_data_uns.median(), inplace=True)

"""- Missing Data:
  - Tenure: 81 values
  - WarehouseToHome: 84 values
  - HourSpendOnApp: 58 values
  - OrderAmoutHikeformLastYear: 14 values
  - CouponUsed: 8 values
  - OrderCount: 18 values
  - DaySinceLastOrder: 54 values
> Action needed: **Filling missing values with respectively median values**
"""

data_uns.nunique()

data_uns.duplicated().any()

"""- Duplicates:
  - Primal Key: CustomerID
  - Duplicates in primal key: None
  - Duplicates in sample: None
> Action needed: **No Action**

### **2.3. Univariate Analysis**
"""

data_uns['CityTier'] = data_uns['CityTier'].astype('object')
data_uns['SatisfactionScore'] = data_uns['SatisfactionScore'].astype('object')

"""- CityTier is a false numeric data without quantitative meanings >> Change back to object data type.
- SatisfactionScore - CityTier is a false numeric data without quantitative meanings >> Change back to object data type.
- CityTier is a false numeric data with quantitative meanings >> Keep int64 data type

#### 2.3.1. Numeric Data
"""

numeric_cols_uns = data_uns.loc[:, data_uns.dtypes != object].columns.tolist()
numeric_cols_uns.remove('CustomerID')
numeric_cols_uns.remove('Churn')

sns.set_theme(style="darkgrid")
for col in numeric_cols_uns:
    print(f"Unique values of {col}: {data_uns[col].nunique()}")
plt.show()
for col in numeric_cols_uns:
    plt.figure(figsize=(16, 8))
    sns.histplot(x=data_uns[col], bins=20, color='#0fbb98')
    plt.title(f'Distribution of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.ylabel('')
plt.show()

for col in numeric_cols_uns:
    plt.figure(figsize=(16, 8))
    sns.boxplot(x=data_uns[col])
    plt.title(f'Boxplot of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.show()

"""- Outliers Detected:
  - Tenure
  - HourSpendOnApp
  - NumberOfAddress
  - OrderAmountHikeFromlastYear
  - CouponUsed
  - OrderCount
  - DaySinceLastOrder
  - CashbackAmount
- Action Needed:
  - All outliers replaced by their respective median values
"""

numeric_cols_uns = data_uns.drop(columns=['HourSpendOnApp']).select_dtypes(include=np.number).columns
for col in numeric_cols_uns:
    Q1_uns = data_uns[col].quantile(0.25)
    Q3_uns = data_uns[col].quantile(0.75)
    IQR_uns = Q3_uns - Q1_uns
    lower_bound_uns = Q1_uns - 1.5 * IQR_uns
    upper_bound_uns = Q3_uns + 1.5 * IQR_uns
    median_uns = data_uns[col].median()
    data_uns[col] = np.where((data_uns[col] < lower_bound_uns) | (data_uns[col] > upper_bound_uns), median_uns, data_uns[col])

"""#### 2.3.2. Category Data"""

cate_cols_uns = data_uns.loc[:, data_uns.dtypes == object].columns.tolist()
for col in cate_cols_uns:
    print(f"Unique values of {col}: {data_uns[col].nunique()}")
for col in cate_cols_uns:
    plt.figure(figsize=(16, 8))
    sns.histplot(x=data_uns[col], bins=20, color='#0fbb98')
    plt.title(f'Distribution of {col}', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
    plt.xlabel('')
    plt.ylabel('')
plt.show()

data_uns['PreferredLoginDevice'] = data_uns['PreferredLoginDevice'].replace('Phone', 'Mobile Phone')
data_uns['PreferedOrderCat'] = data_uns['PreferedOrderCat'].replace('Mobile', 'Mobile Phone')
data_uns['PreferredPaymentMode'] = data_uns['PreferredPaymentMode'].replace('CC', 'Credit Card')
data_uns['PreferredPaymentMode'] = data_uns['PreferredPaymentMode'].replace('COD', 'Cash on Delivery')

"""- As there are 2 values with similar meaning in label PreferredLoginDevice ('Mobile Phone' and 'Phone'), all of them will be change into 'Mobile Phone'.
- As there are 2 values with similar meaning in label PreferedOrderCat ('Mobile Phone' and 'Mobile'), all of them will be change into 'Mobile Phone'.
- As there are 2 values with similar meaning in label PreferredPaymentMode ('CC' and 'Credit Card'), all of them will be change into 'Credit Card'.
- As there are 2 values with similar meaning in label PreferredPaymentMode ('COD' and 'Cash on Delivery'), all of them will be change into 'Cash on Delivery'.

### **2.4. Bivariate & Multivariate Analysis**

#### Correlation Coefficient
"""

numeric_cols_uns = data_uns.loc[:, data_uns.dtypes != object].columns.tolist()
numeric_cols_uns.remove('CustomerID')
numeric_cols_uns.remove('Churn')
plt.figure(figsize=(16, 8))
sns.heatmap(data_uns[numeric_cols_uns].corr(), cmap='Greens', annot=True)
plt.show()

"""## **3. Feature Engineering & Feature Transforming**

### **3.1. Select Potential Features**
"""

df_model_uns= data_uns.drop(columns=['CustomerID', 'Churn'])
df_model_uns

"""### **3.2. Encoding**"""

dummies_df_uns = pd.get_dummies(df_model_uns,columns = ['PreferredLoginDevice','PreferredPaymentMode','Gender','PreferedOrderCat','MaritalStatus'], drop_first=True)

dummies_df_uns.head(5)

"""### **3.3. Normalization**"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
normalized_data_uns = scaler.fit_transform(dummies_df_uns)
normalized_df_uns = pd.DataFrame(normalized_data_uns, columns=dummies_df_uns.columns)

"""- As the results of normalization cannot be used in Dimension Reduction (PCA) due to low explained variance rate, the this step is only shown as an example.

### **3.4. Dimension Reduction**
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(dummies_df_uns)
PCA_ds = pd.DataFrame(pca.transform(dummies_df_uns), columns=(["col1", "col2", "col3"]))
PCA_ds

pca.explained_variance_ratio_

"""## **4. Model Training - Apply K-Means Model**

### **4.1. Choosing K**
"""

from sklearn.cluster import KMeans
ss = []
max_clusters = 10
for i in range(1, max_clusters+1):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(PCA_ds)
    ss.append(kmeans.inertia_)
plt.figure(figsize=(16,8))
plt.plot(range(1, max_clusters+1), ss, marker='o', linestyle='--', color='#0fbb98')
plt.title('Elbow Method', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""- K = 4 will be chosen for the model.

### **4.2. Applying K-Means**
"""

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
predicted_labels = kmeans.fit_predict(PCA_ds)
PCA_ds['clusters']=predicted_labels
data_uns['clusters']=predicted_labels
data_uns.head(5)

"""## **5.Model Evaluation**"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import AgglomerativeClustering
from matplotlib.colors import ListedColormap

"""### **5.1. Silhouette Score**"""

from sklearn.metrics import silhouette_score
sil_score = silhouette_score(PCA_ds, predicted_labels)
print(sil_score)

"""### **5.2. Distribution Of Clusters**"""

plt.figure(figsize=(16,8))
pl = sns.countplot(x=data_uns["clusters"], color='#0fbb98')
pl.set_title("Distribution Of The Clusters", fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.show()

"""## **6. Analyzation**

- To get information about the clusters, a Supervised model is applied.

### Select Potential Features
"""

df_model_uns= data_uns.drop(columns=['CustomerID', 'Churn'])
df_model_uns.head()

"""### Encoding"""

list_columns_uns = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender','PreferedOrderCat','MaritalStatus']
df_encoded_uns = pd.get_dummies(df_model_uns, columns = list_columns_uns)
df_encoded_uns.head()

"""### Split Dataset into Train/Test"""

x=df_encoded_uns.drop('clusters', axis = 1)
y=df_encoded_uns[['clusters']]
x_train_uns, x_test_uns, y_train_uns, y_test_uns = train_test_split(x, y, test_size=0.3, random_state=42)
print(f"Number of Data in Train Set: {len(x_train_uns)}")
print(f"Number of Data in Test Set: {len(x_test_uns)}")

"""### Normalization"""

scaler = MinMaxScaler()
scaler.fit(x_train_uns)
x_train_scaled_uns = scaler.transform(x_train_uns)
x_test_scaled_uns = scaler.transform(x_test_uns)

"""### Model Training - Random Forest"""

from sklearn.ensemble import RandomForestClassifier

clf_rand_uns = RandomForestClassifier(max_depth=2, random_state=42)

clf_rand_uns.fit(x_train_scaled_uns, y_train_uns)
y_ranf_pre_train_uns = clf_rand_uns.predict(x_train_scaled_uns)
y_ranf_pre_test_uns = clf_rand_uns.predict(x_test_scaled_uns)

"""### Model Evaluation"""

from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(y_test_uns, y_ranf_pre_test_uns) * 100
print(f"Accuracy on Test: {test_accuracy:.2f}%")

"""### Feature Importance"""

feats = {}
for feature, importance in zip(x_test_uns.columns, clf_rand_uns.feature_importances_):
    feats[feature] = importance
importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})
importances = importances.sort_values(by='Gini-importance', ascending=True)
importances = importances.reset_index()
plt.figure(figsize=(16, 9))
plt.barh(importances.tail(20)['index'][:20], importances.tail(20)['Gini-importance'])
plt.title('Feature Important', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.show()

"""Top features with greatest effect on deciding churned users, in order:
- CashbackAmount
- OrderCount
- PreferedOrderCat

### Cluster Analyzation
"""

plt.figure(figsize=(16, 8))
sns.kdeplot(data=data_uns, x='CashbackAmount', hue='clusters', fill=True, palette={0: 'red', 1: 'blue', 2: 'green', 3: 'purple'})
plt.title('Distribution of Cashback Amount Across Clusters', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('Cashback Amount', fontsize=12)
plt.ylabel('Density', fontsize=12)
plt.show()

"""- Features of each cluster in Cashback Value:
  - Cluster 0: Distributed mostly on 145 - 155
  - Cluster 1: Distributed equally on 200 - 230
  - Cluster 2: Distributed mostly on 120 - 130
  - Cluster 3: Distributed mostly on 165 - 190
- Conclusion
  - Cluster 2 and 0: Low average cashback amount in last month
  - Cluster 3: Medium average cashback amout in last month
  - Cluster 1: High average cashback amount in last month
"""

plt.figure(figsize=(14, 7))
sns.countplot(x='OrderCount', hue='clusters', data=data_uns, palette='Set1')
plt.title('Order Count Distribution Across Clusters', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.show()

"""- Features of each cluster in Order Count:
  - Cluster 0: Distributed mostly on 2 orders, and there are some values in 1 and 3 orders.
  - Cluster 1: Distributed mostly on 2 orders, and there are some values in 3, 4 and 5 orders
  - Cluster 2: Distributed mostly on 1 order, and there are some values in 2 order.
  - Cluster 3: Distributed mostly on 2 orders, and there are some values in 1, 3 and 4 orders.
- Conclusion
  - Cluster 0: Lower number of orders placed in last month (1-3)
  - Cluster 2: Lower number of orders placed in last month (1-2)
  - Cluster 3: Higher number of orders placed in last month (2-4)
  - Cluster 1: Higher number of orders placed in last month (2-5)
"""

plt.figure(figsize=(14, 7))
sns.countplot(x='PreferedOrderCat', hue='clusters', data=data_uns, palette='Set1')
plt.title('Prefered Order Category Distribution Across Clusters', fontdict={'fontsize': 20, 'fontweight': 'bold'}, loc='center', pad=12)
plt.xlabel('')
plt.show()

"""- Features of each cluster in Prefered Order Category:
  - Cluster 0: Distributed mostly on mobile phone and laptop & accessory
  - Cluster 1: Distributed mostly on laptop & accessory and fashion
  - Cluster 2: Distributed mostly on mobile phone
  - Cluster 3: Distributed mostly on laptop & accessory, mobile phone and fashion

# **III. Conclusion**

1. Common features of churned users:
  - The users who **have less that 15 months of tenure**
  - The users who **complained in the last month**
  - The users who **login by computer**
  - The users who **stay in city tier 2 or 3**
  - The users who **pay by Cash on Delivery or E-wallet**
  - The users who **had preferred order category of Fashion or Mobile Phone**
  - The users **with big satisfaction score**
  - The users who **are currently single**
2. Suggestion to reduce churned users:
- Factors of users that cannot be changed or improved by the company: **MaritalStatus, Tenure**
- The other factors can be improved by the company to reduce the number of churn users:
  - **Complain**: improve the quality of product and customer service, solve the problems as they rise so the customers will be satisfied and may not complain.
  - **LoginDevice**: test and fix any bugs or technical problems that affect the experience of customers login using computer
  - **CityTier**: invest more on marketing in city tier 1, reduce investment on city tier 2 and 3.
  - **PaymentMethod**: find and resolve any inconvenience in using Cash on Delivery and E-wallet to make purchase, in order to improve the purchasing experience of users in these payment methods.
  - **Preferred Category**: improve the quality of product in Fashion and Mobile Phone segments.
  - **Satisfaction Score**: it is needed to check if the score is in an ascending or descending order. In case the score is sorted in ascending order, this is a strange phenomenon that should be monitoring further.
3. Supervised Machine Learning Model:
- Random Forest: High Precision, Fitting & Effective
- Accuracy of Random Forest Fine-Tuned: **91.72%**
4. Unsupervised Machine Learning Model:
- K-means Model. K = **4**
- Silhouetter Score: **0.46**
- Differencies between 4 groups of customers:
  - **Group 1 (Cluster 0)** - people with:
    - Low average cashback amount in last month (145-155).
    - Lower number of orders placed in last month (from 1-3 orders).
    - Prefer mobile phone and laptop & accessory in last month.
  - **Group 2 (Cluster 1)** - people with:
    - High average cashback amount in last month (200 - 230).
    - Higher number of orders placed last month (from 2-5 orders).
    - Prefer laptop & accessory and fashion in the last month.
  - **Group 3 (Cluster 2)** - people with:
    - Low average cashback amount in last month (120 - 130).
    - Lower number of orders placed last month (from 1-2 orders).
    - Prefer mobile phone in the last month.
  - **Group 4 (Cluster 3)** - people with:
    - Medium average cashback amount in last month (165 - 190).
    - Higher number of orders placed last month (from 2-4 orders).
    - Prefer laptop & accessory, mobile phone and fashion in the last month.
"""